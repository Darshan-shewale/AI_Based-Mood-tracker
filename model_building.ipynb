{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07ba9f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Darshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Darshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Darshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Darshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Download NLTK resources (only needed once)\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_test = pd.read_csv('emotions\\\\test.csv')\n",
    "df_train = pd.read_csv('emotions\\\\training.csv') \n",
    "df_validation = pd.read_csv('emotions\\\\validation.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5b1577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_test Shape:  (2000, 2)\n",
      "DF_train Shape:  (16000, 2)\n",
      "DF_validation Shape:  (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape (# of rows and columns) of the datasets\n",
    "print('DF_test Shape: ', df_test.shape)\n",
    "print('DF_train Shape: ', df_train.shape)\n",
    "print('DF_validation Shape: ', df_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655312e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <<< DATASET 1 df_test -----------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2000 non-null   object\n",
      " 1   label   2000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      " <<< DATASET 2 df_train-----------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    16000 non-null  object\n",
      " 1   label   16000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 250.1+ KB\n",
      "None\n",
      " <<< DATASET 3 df_validation-----------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2000 non-null   object\n",
      " 1   label   2000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# checking the info (columns, datatypes, nulls) of the datasets\n",
    "print(' <<< DATASET 1 df_test -----------------------------------------------------------')\n",
    "print(df_test.info())\n",
    "print(' <<< DATASET 2 df_train-----------------------------------------------------------')\n",
    "print(df_train.info())\n",
    "print(' <<< DATASET 3 df_validation-----------------------------------------------------------')\n",
    "print(df_validation.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0168eba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "15995    i just had a very brief time in the beanbag an...\n",
       "15996    i am now turning and i feel pathetic that i am...\n",
       "15997                       i feel strong and good overall\n",
       "15998    i feel like this was such a rude comment and i...\n",
       "15999    i know a lot but i feel so stupid because i ca...\n",
       "Name: text, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91573781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess comments\n",
    "def preprocess_comment(comment):\n",
    "    if isinstance(comment, str):\n",
    "        comment = comment.lower()\n",
    "        comment = re.sub(r'http\\S+', '', comment)\n",
    "        comment = re.sub(r'[^a-zA-Z\\s]', '', comment)\n",
    "        tokens = word_tokenize(comment)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        cleaned_comment = ' '.join(lemmatized_tokens)\n",
    "    else:\n",
    "        cleaned_comment = ''\n",
    "    return cleaned_comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12841eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_comments'] = df_train['text'].apply(preprocess_comment)\n",
    "df_test['cleaned_comments'] = df_test['text'].apply(preprocess_comment)\n",
    "df_validation['cleaned_comments'] = df_validation['text'].apply(preprocess_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7faeff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_test :                                                  text  label  \\\n",
      "0  im feeling rather rotten so im not very ambiti...      0   \n",
      "1          im updating my blog because i feel shitty      0   \n",
      "2  i never make her separate from me because i do...      0   \n",
      "3  i left with my bouquet of red and yellow tulip...      1   \n",
      "4    i was feeling a little vain when i did this one      0   \n",
      "\n",
      "                                    cleaned_comments  \n",
      "0        im feeling rather rotten im ambitious right  \n",
      "1                       im updating blog feel shitty  \n",
      "2    never make separate ever want feel like ashamed  \n",
      "3  left bouquet red yellow tulip arm feeling slig...  \n",
      "4                            feeling little vain one  \n",
      "DF_train :                                                  text  label  \\\n",
      "0                            i didnt feel humiliated      0   \n",
      "1  i can go from feeling so hopeless to so damned...      0   \n",
      "2   im grabbing a minute to post i feel greedy wrong      3   \n",
      "3  i am ever feeling nostalgic about the fireplac...      2   \n",
      "4                               i am feeling grouchy      3   \n",
      "\n",
      "                                    cleaned_comments  \n",
      "0                              didnt feel humiliated  \n",
      "1  go feeling hopeless damned hopeful around some...  \n",
      "2          im grabbing minute post feel greedy wrong  \n",
      "3  ever feeling nostalgic fireplace know still pr...  \n",
      "4                                    feeling grouchy  \n",
      "DF_validation :                                                  text  label  \\\n",
      "0  im feeling quite sad and sorry for myself but ...      0   \n",
      "1  i feel like i am still looking at a blank canv...      0   \n",
      "2                     i feel like a faithful servant      2   \n",
      "3                  i am just feeling cranky and blue      3   \n",
      "4  i can have for a treat or if i am feeling festive      1   \n",
      "\n",
      "                                    cleaned_comments  \n",
      "0           im feeling quite sad sorry ill snap soon  \n",
      "1  feel like still looking blank canvas blank pie...  \n",
      "2                         feel like faithful servant  \n",
      "3                                feeling cranky blue  \n",
      "4                              treat feeling festive  \n"
     ]
    }
   ],
   "source": [
    "print('DF_test : ', df_test.head())\n",
    "print('DF_train : ', df_train.head())\n",
    "print('DF_validation : ', df_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06e8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the max_features as needed\n",
    "X_train = tfidf_vectorizer.fit_transform(df_train['text'])\n",
    "X_val = tfidf_vectorizer.transform(df_validation['text'])\n",
    "X_test = tfidf_vectorizer.transform(df_test['text'])\n",
    "y_train = df_train['label']\n",
    "y_val = df_validation['label']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7abefdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the sentiment analysis model (Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20135086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f'Validation Accuracy: {validation_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "362f0bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and vectorizer for later use\n",
    "joblib.dump(model, 'sentiment_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
